---
###############################################################################################
# This role deletes an CEPH OSD from a running CEPH cluster.
# Note: This is for our specific setup with only one ceph-osd daemon (== disk) per server.
# It removes the packages, configurations and all data from the OSD.
# It uses the exOSD inventory group from the Ansible inventory.
# For further info, see: http://docs.ceph.com/docs/hammer/rados/operations/add-or-rm-osds/.
###############################################################################################

- name: Check OSD's on this host.
  script: osds_on_this_host.sh
  register: host_osds
  failed_when: host_osds.rc == 2

- name: Check the health of the CEPH cluster.
  script: wait_for_health_ok.sh 60
#  command: ceph health
# works, but wait_for_health_ok.sh waits until, for x seconds...

- name: Check before we act
  debug: msg="In a future version, we might test here how much GB we are detaching and if there is enough free space before we continue. Not yet implemented though."

- name: Still healthy? If not, lets just break and let people decide why we're in a stable enough situation to continue...
  command: /usr/bin/ceph health

- name: Take OSD out of cluster
  command: "/usr/bin/ceph osd out {{item}}"
  with_items: '{{host_osds.stdout_lines}}'

# wait_for_health_ok.sh 300 will check every second for 300 secs (5 mins)
# after which ansible will display something (so that you think there might be progression...)
# retries 12 will try this for 12 times, so wait for an hour until it fails.
# An hour might seem much, but taking out many nodes with many disks might have a huge impact...
# Tuning might be required when more experience is achieved...

- name: Wait for rebalance to finish
  script: wait_for_health_ok.sh 300
  retries: 12

- name: find services
  script: ceph_services.sh
  register: ceph_services
  failed_when: host_osds.rc == 2

- name: stop service
  service: name="{{ item }}" state=stopped enabled=no
  with_items: '{{ ceph_services.stdout_lines }}'

- name: osd folders
  shell: "ls -d /var/lib/ceph/osd/{{ ceph_cluster_name }}-*"
  register: osd_folders
  ignore_errors: yes

- name: umount osd folders
  shell: "umount {{ item }}"
  with_items: "{{ osd_folders.stdout_lines }}"
  ignore_errors: yes

- name: remove line from fstab
  script: clean_fstab.sh "{{ item }}"
  with_items: "{{ osd_folders.stdout_lines }}"

- name: find ceph data partitions
  script: ceph_data_partitions.py
  register: ceph_data_partitions

- name: empty osd disk
  script: empty_osd_disk.sh "{{item}}"
  with_items: '{{ ceph_data_partitions.stdout_lines }}'

- name: remove folder
  file: dest="{{ item }}" state=absent
  with_items: '{{ osd_folders.stdout_lines }}'

- name: Crush remove osds
  command: /usr/bin/ceph osd crush remove "{{ item }}"
  with_items: '{{ host_osds.stdout_lines }}'

- name: Crush remove host
  command: /usr/bin/ceph osd crush remove "{{ ansible_hostname }}"

- name: Remove Authkey
  command: /usr/bin/ceph auth del "{{ item }}"
  with_items: '{{ host_osds.stdout_lines }}'

- name: Remove OSD
  command: /usr/bin/ceph osd rm "{{ item }}"
  with_items: '{{ host_osds.stdout_lines }}'

- name: update master ceph.conf
  debug: msg="In our tests there was no master ceph.conf file to be identified. Furthermore we don't add keys there either. Skipping for now..."

- name: update master ceph.conf
  debug: msg="Somehow, something goes terribly wrong after removal and before reboot if you add again. partitioning disk creates folder in /var/lib/ceph/osd/ceph-#/. Please reboot before using add again."

#- name: Check the status of the CEPH cluster.
#  command: ceph status


